{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "053d6826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40c48822",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Environmental Discourse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91705a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = pd.read_pickle('../Data/'+path+'/env_0.pkl')\n",
    "env = env.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dd50a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20339</th>\n",
       "      <td>Resilience</td>\n",
       "      <td>https://www.resilience.org/stories/2008-07-08/...</td>\n",
       "      <td>Asia – July 8</td>\n",
       "      <td>2008-07-08</td>\n",
       "      <td>Staff</td>\n",
       "      <td>.     Japan sees a chance to promote its energ...</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87817</th>\n",
       "      <td>Grist</td>\n",
       "      <td>https://grist.org/article/the-blame-the-enviro...</td>\n",
       "      <td>A possible smear campaign fingers greens...</td>\n",
       "      <td>2005-09-17</td>\n",
       "      <td>Emily Gertz</td>\n",
       "      <td>The Gonzales Justice Department may be seeking...</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63974</th>\n",
       "      <td>Grist</td>\n",
       "      <td>https://grist.org/climate/heres-how-coronaviru...</td>\n",
       "      <td>Here’s how coronavirus affected carbon e...</td>\n",
       "      <td>2020-05-19</td>\n",
       "      <td>Zoya Teirstein</td>\n",
       "      <td>The pandemic is far from over, but some states...</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94514</th>\n",
       "      <td>Grist</td>\n",
       "      <td>https://grist.org/article/2011-01-21-meet-the-...</td>\n",
       "      <td>Meet the Climate Fockers</td>\n",
       "      <td>2011-01-22</td>\n",
       "      <td>Auden Schendler</td>\n",
       "      <td>Here’a repost from my Climateprogress.org blog...</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57887</th>\n",
       "      <td>Grist</td>\n",
       "      <td>https://grist.org/living/3-disturbing-facts-pr...</td>\n",
       "      <td>3 disturbing facts prove sexual harassme...</td>\n",
       "      <td>2015-04-16</td>\n",
       "      <td>Eve Andrews</td>\n",
       "      <td>Hello! On this spring-shiny day, are you think...</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           source                                                url  \\\n",
       "20339  Resilience  https://www.resilience.org/stories/2008-07-08/...   \n",
       "87817       Grist  https://grist.org/article/the-blame-the-enviro...   \n",
       "63974       Grist  https://grist.org/climate/heres-how-coronaviru...   \n",
       "94514       Grist  https://grist.org/article/2011-01-21-meet-the-...   \n",
       "57887       Grist  https://grist.org/living/3-disturbing-facts-pr...   \n",
       "\n",
       "                                                   title       date  \\\n",
       "20339                                      Asia – July 8 2008-07-08   \n",
       "87817        A possible smear campaign fingers greens... 2005-09-17   \n",
       "63974        Here’s how coronavirus affected carbon e... 2020-05-19   \n",
       "94514                       Meet the Climate Fockers     2011-01-22   \n",
       "57887        3 disturbing facts prove sexual harassme... 2015-04-16   \n",
       "\n",
       "                 author                                               text  \\\n",
       "20339             Staff  .     Japan sees a chance to promote its energ...   \n",
       "87817      Emily Gertz   The Gonzales Justice Department may be seeking...   \n",
       "63974   Zoya Teirstein   The pandemic is far from over, but some states...   \n",
       "94514  Auden Schendler   Here’a repost from my Climateprogress.org blog...   \n",
       "57887      Eve Andrews   Hello! On this spring-shiny day, are you think...   \n",
       "\n",
       "       year  \n",
       "20339  2008  \n",
       "87817  2005  \n",
       "63974  2020  \n",
       "94514  2011  \n",
       "57887  2015  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea33a53",
   "metadata": {},
   "source": [
    "# Sentence tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a03cd3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from gensim.utils import effective_n_jobs\n",
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load(\"en\")\n",
    "except OSError:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84eaca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(word_list, model=nlp, MAX_LEN=1500000):\n",
    "    \n",
    "    tokenized = []\n",
    "    if type(word_list) == list and len(word_list) == 1:\n",
    "        word_list = word_list[0]\n",
    "\n",
    "    if type(word_list) == list:\n",
    "        word_list = ' '.join([str(elem) for elem in word_list]) \n",
    "    # since we're only tokenizing, I remove RAM intensive operations and increase max text size\n",
    "\n",
    "    model.max_length = MAX_LEN\n",
    "    doc = model(word_list, disable=[\"parser\", \"tagger\", \"ner\", \"lemmatizer\"])\n",
    "    \n",
    "    for token in doc:\n",
    "        if not token.is_punct and len(token.text.strip()) > 0:\n",
    "            tokenized.append(token.text)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b38a77f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeTokens(word_list, extra_stop=[], model=nlp, lemma=True, MAX_LEN=1500000):\n",
    "    #We can use a generator here as we just need to iterate over it\n",
    "    normalized = []\n",
    "    if type(word_list) == list and len(word_list) == 1:\n",
    "        word_list = word_list[0]\n",
    "\n",
    "    if type(word_list) == list:\n",
    "        word_list = ' '.join([str(elem) for elem in word_list]) \n",
    "\n",
    "    # since we're only normalizing, I remove RAM intensive operations and increase max text size\n",
    "\n",
    "    model.max_length = MAX_LEN\n",
    "    doc = model(word_list.lower(), disable=[\"parser\", \"ner\"])\n",
    "\n",
    "    if len(extra_stop) > 0:\n",
    "        for stopword in extra_stop:\n",
    "            lexeme = nlp.vocab[stopword]\n",
    "            lexeme.is_stop = True\n",
    "\n",
    "    # we check if we want lemmas or not earlier to avoid checking every time we loop\n",
    "    if lemma:\n",
    "        for w in doc:\n",
    "            # if it's not a stop word or punctuation mark, add it to our article\n",
    "            if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and len(w.text.strip()) > 0:\n",
    "            # we add the lematized version of the word\n",
    "                normalized.append(str(w.lemma_))\n",
    "    else:\n",
    "        for w in doc:\n",
    "            # if it's not a stop word or punctuation mark, add it to our article\n",
    "            if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and len(w.text.strip()) > 0:\n",
    "            # we add the lematized version of the word\n",
    "                normalized.append(str(w.text.strip()))\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6f2895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenize(word_list, model=nlp):\n",
    "    doc = model(word_list)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23152aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_env = dd.from_pandas(env, npartitions=effective_n_jobs(-1))\n",
    "env['tokenized_sents'] = env.text.apply(lambda x: [word_tokenize(s) for s in sent_tokenize(x)])\n",
    "env['normalized_sents'] = env['tokenized_sents'].apply(lambda x: [normalizeTokens(s, lemma=False) for s in x])\n",
    "env = d_env.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f74359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12f6fb5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 7)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8e474e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_env = dd.from_pandas(env_tok, npartitions=effective_n_jobs(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6591623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22bcb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bfc690",
   "metadata": {},
   "outputs": [],
   "source": [
    "envW2V_skipgram = gensim.models.word2vec.Word2Vec(env['normalized_sents'].sum(), sg=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
