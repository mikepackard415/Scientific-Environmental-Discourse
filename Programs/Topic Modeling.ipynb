{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a5bb098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import effective_n_jobs\n",
    "import ast\n",
    "\n",
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load(\"en\")\n",
    "except OSError:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0a866f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.3\n"
     ]
    }
   ],
   "source": [
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67272c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = pd.read_csv('../Data/Environmental Discourse/env.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8510f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "env['date'] = pd.to_datetime(env.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37d80640",
   "metadata": {},
   "outputs": [],
   "source": [
    "env['year'] = env.date.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ee6d3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "small = env.groupby('year').sample(100, random_state=3291995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c361a141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65099</th>\n",
       "      <td>Grist</td>\n",
       "      <td>https://grist.org/article/im-lovin-it/</td>\n",
       "      <td>I’m lovin’ it!</td>\n",
       "      <td>2005-02-23</td>\n",
       "      <td>David Roberts</td>\n",
       "      <td>Okay, sorry I put that song in your head.  Thi...</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73994</th>\n",
       "      <td>Grist</td>\n",
       "      <td>https://grist.org/article/the-atlantic-ocean-i...</td>\n",
       "      <td>The Atlantic Ocean is going to kill you</td>\n",
       "      <td>2005-12-01</td>\n",
       "      <td>David Roberts</td>\n",
       "      <td>Speaking of Oil Drum, they remind me to point ...</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60665</th>\n",
       "      <td>Grist</td>\n",
       "      <td>https://grist.org/article/girl-you-trippin/</td>\n",
       "      <td>Girl, You Trippin’!</td>\n",
       "      <td>2005-03-29</td>\n",
       "      <td>Grist staff</td>\n",
       "      <td>Cameron, Arnold lead brigade of celeb eco-spok...</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39378</th>\n",
       "      <td>EMagazine</td>\n",
       "      <td>https://emagazine.com/bombing-bali-hai/</td>\n",
       "      <td>Bombing Bali Ha’i</td>\n",
       "      <td>2005-03-16</td>\n",
       "      <td>From the Editors of E Magazine</td>\n",
       "      <td>We Love Tropical Islands, But That Doesn’t Pre...</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56527</th>\n",
       "      <td>Grist</td>\n",
       "      <td>https://grist.org/article/going-it-alone/</td>\n",
       "      <td>Vashon Island goes energy independent</td>\n",
       "      <td>2005-08-25</td>\n",
       "      <td>Andy Brett</td>\n",
       "      <td>From the Seattle PI: Vashon Island, located ju...</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          source                                                url  \\\n",
       "65099      Grist             https://grist.org/article/im-lovin-it/   \n",
       "73994      Grist  https://grist.org/article/the-atlantic-ocean-i...   \n",
       "60665      Grist        https://grist.org/article/girl-you-trippin/   \n",
       "39378  EMagazine            https://emagazine.com/bombing-bali-hai/   \n",
       "56527      Grist          https://grist.org/article/going-it-alone/   \n",
       "\n",
       "                                                   title       date  \\\n",
       "65099                                 I’m lovin’ it!     2005-02-23   \n",
       "73994        The Atlantic Ocean is going to kill you     2005-12-01   \n",
       "60665                            Girl, You Trippin’!     2005-03-29   \n",
       "39378                                  Bombing Bali Ha’i 2005-03-16   \n",
       "56527          Vashon Island goes energy independent     2005-08-25   \n",
       "\n",
       "                               author  \\\n",
       "65099                  David Roberts    \n",
       "73994                  David Roberts    \n",
       "60665                    Grist staff    \n",
       "39378  From the Editors of E Magazine   \n",
       "56527                     Andy Brett    \n",
       "\n",
       "                                                    text  year  \n",
       "65099  Okay, sorry I put that song in your head.  Thi...  2005  \n",
       "73994  Speaking of Oil Drum, they remind me to point ...  2005  \n",
       "60665  Cameron, Arnold lead brigade of celeb eco-spok...  2005  \n",
       "39378  We Love Tropical Islands, But That Doesn’t Pre...  2005  \n",
       "56527  From the Seattle PI: Vashon Island, located ju...  2005  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c463371e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1700, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91588650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(word_list, model=nlp, MAX_LEN=1500000):\n",
    "    \n",
    "    tokenized = []\n",
    "    if type(word_list) == list and len(word_list) == 1:\n",
    "        word_list = word_list[0]\n",
    "\n",
    "    if type(word_list) == list:\n",
    "        word_list = ' '.join([str(elem) for elem in word_list]) \n",
    "    # since we're only tokenizing, I remove RAM intensive operations and increase max text size\n",
    "\n",
    "    model.max_length = MAX_LEN\n",
    "    doc = model(word_list, disable=[\"parser\", \"tagger\", \"ner\", \"lemmatizer\"])\n",
    "    \n",
    "    for token in doc:\n",
    "        if not token.is_punct and len(token.text.strip()) > 0:\n",
    "            tokenized.append(token.text)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "292fbb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeTokens(word_list, extra_stop=[], model=nlp, lemma=True, MAX_LEN=1500000):\n",
    "    #We can use a generator here as we just need to iterate over it\n",
    "    normalized = []\n",
    "    if type(word_list) == list and len(word_list) == 1:\n",
    "        word_list = word_list[0]\n",
    "\n",
    "    if type(word_list) == list:\n",
    "        word_list = ' '.join([str(elem) for elem in word_list]) \n",
    "\n",
    "    # since we're only normalizing, I remove RAM intensive operations and increase max text size\n",
    "\n",
    "    model.max_length = MAX_LEN\n",
    "    doc = model(word_list.lower(), disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "\n",
    "    if len(extra_stop) > 0:\n",
    "        for stopword in extra_stop:\n",
    "            lexeme = nlp.vocab[stopword]\n",
    "            lexeme.is_stop = True\n",
    "\n",
    "    # we check if we want lemmas or not earlier to avoid checking every time we loop\n",
    "    if lemma:\n",
    "        for w in doc:\n",
    "            # if it's not a stop word or punctuation mark, add it to our article\n",
    "            if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and len(w.text.strip()) > 0:\n",
    "            # we add the lematized version of the word\n",
    "                normalized.append(str(w.lemma_))\n",
    "    else:\n",
    "        for w in doc:\n",
    "            # if it's not a stop word or punctuation mark, add it to our article\n",
    "            if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and len(w.text.strip()) > 0:\n",
    "            # we add the lematized version of the word\n",
    "                normalized.append(str(w.text.strip()))\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b77ac36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "526c493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out how to parallelize the different topic models\n",
    "# Thinking 4, 6, 8, and 10 topics in each of 2007, 2013, and 2019\n",
    "# For now, I need to write the code to replace bigrams with bi_grams, ja feel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e17eff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "quadgrams = [('intergovernmental', 'panel', 'climate', 'change'),\n",
    "             ('natural', 'resources', 'defense', 'council'),\n",
    "             ('coal', 'fired', 'power', 'plants'),\n",
    "             ('national', 'oceanic', 'atmospheric', 'administration')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0cdc21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.read_csv('../Data/Environmental Discourse/trigrams.csv', converters={'Unnamed: 0': ast.literal_eval})\n",
    "tr.columns = ['trigram', 'freq', 'tag']\n",
    "trigrams = [t for t in tr[tr.tag == 1].trigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ed0806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.read_csv('../Data/Environmental Discourse/bigrams.csv', converters={'Unnamed: 0': ast.literal_eval})\n",
    "b.columns = ['bigram', 'freq', 'tag']\n",
    "bigrams = [t for t in b[b.tag == 1].bigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bc051ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "quadgrams = ['_'.join(t) for t in quadgrams]\n",
    "trigrams = ['_'.join(t) for t in trigrams]\n",
    "bigrams = ['_'.join(t) for t in bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b3df049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_tagger(tokens):\n",
    "    n = len(tokens)\n",
    "    i = 0\n",
    "    tokens_q = []\n",
    "    tokens_qt = []\n",
    "    tokens_qtb = []\n",
    "    \n",
    "    # quadgrams\n",
    "    while i < n:\n",
    "        words = '_'.join(tokens[i:i+4])\n",
    "        if words in quadgrams:\n",
    "            tokens_q.append(words)\n",
    "            i += 4\n",
    "        else:\n",
    "            tokens_q.append(tokens[i])\n",
    "            i += 1\n",
    "    \n",
    "    # trigrams\n",
    "    n = len(tokens_q)\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        words = '_'.join(tokens_q[i:i+3])\n",
    "        if words in trigrams:\n",
    "            tokens_qt.append(words)\n",
    "            i += 3\n",
    "        else:\n",
    "            tokens_qt.append(tokens_q[i])\n",
    "            i += 1\n",
    "    \n",
    "    # bigrams\n",
    "    n = len(tokens_qt)\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        words = '_'.join(tokens_qt[i:i+2])\n",
    "        if words in bigrams:\n",
    "            tokens_qtb.append(words)\n",
    "            i += 2\n",
    "        else:\n",
    "            tokens_qtb.append(tokens_qt[i])\n",
    "            i += 1\n",
    "    \n",
    "    return tokens_qtb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5a0ca7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "small = env.groupby('year').sample(100, random_state=3291995)\n",
    "dsmall = dd.from_pandas(small, npartitions=effective_n_jobs(-1))\n",
    "dsmall['normalized_tokens'] = dsmall.text.map(lambda x: ngram_tagger(\n",
    "                                                            normalizeTokens(\n",
    "                                                                word_tokenize(x))), meta=('x', str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7efd25db",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_tok = dsmall.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44a8e84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30cb6242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a20589ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary([i for i in small_tok.normalized_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d6d2d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in small_tok.normalized_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c7c3937",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b77f2d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f720839b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.interfaces.TransformedCorpus"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tfidf[bow_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5e4ec295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, 11, 2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40d8eb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=2):\n",
    "    '''\n",
    "    Computes Coherence values for LDA models with differing numbers of topics.\n",
    "    \n",
    "    Returns list of models along with their respective coherence values (pick\n",
    "    models with the highest coherence)\n",
    "    '''\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                                 id2word=dictionary,\n",
    "                                                 num_topics=num_topics,\n",
    "                                                 )\n",
    "        model_list.append(model)\n",
    "        coherence_model = models.coherencemodel.CoherenceModel(model=model, \n",
    "                                                               corpus=corpus,\n",
    "                                                               dictionary=dictionary,\n",
    "                                                               coherence='u_mass')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6545eed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_07 = small_tok.year == 2007\n",
    "#tfidf[bow_corpus[mask_07]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8bd0e706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-11:\n",
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-12:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-16:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n"
     ]
    }
   ],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, \n",
    "                                                        corpus=bow_corpus, \n",
    "                                                        texts=small.normalized_tokens,\n",
    "                                                        start=4, limit=10, step=2)\n",
    "plt.plot(range(2, 40, 6), coherence_values)\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22ef78a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.ldamulticore.LdaMulticore(corpus=bow_corpus,\n",
    "                                         id2word=dictionary,\n",
    "                                         num_topics=4,\n",
    "                                         workers=effective_n_jobs(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c162f02a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2140/362567082.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmylist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmylist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "mylist = [1,2,3]\n",
    "mylist[True, False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4b121ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        False\n",
       "49       False\n",
       "89       False\n",
       "90       False\n",
       "133      False\n",
       "         ...  \n",
       "95706    False\n",
       "95715    False\n",
       "95763    False\n",
       "95776    False\n",
       "95787    False\n",
       "Name: year, Length: 1700, dtype: bool"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11818e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_07 = small_tok.year == 2007\n",
    "mask_13 = small_tok.year == 2013\n",
    "mask_19 = small_tok.year == 2019\n",
    "\n",
    "bow_corpus_07 = [doc for i, doc in enumerate(bow_corpus) if mask_07.iloc[i]]\n",
    "bow_corpus_13 = [doc for i, doc in enumerate(bow_corpus) if mask_13.iloc[i]]\n",
    "bow_corpus_19 = [doc for i, doc in enumerate(bow_corpus) if mask_19.iloc[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "324c3a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_07, coherence_07 = compute_coherence_values(dictionary=dictionary, \n",
    "                                                   corpus=tfidf[bow_corpus_07], \n",
    "                                                   texts=small_tok.normalized_tokens,\n",
    "                                                   start=4, limit=11, step=2)\n",
    "\n",
    "models_13, coherence_13 = compute_coherence_values(dictionary=dictionary, \n",
    "                                                   corpus=tfidf[bow_corpus_13], \n",
    "                                                   texts=small_tok.normalized_tokens,\n",
    "                                                   start=4, limit=11, step=2)\n",
    "\n",
    "models_19, coherence_19 = compute_coherence_values(dictionary=dictionary, \n",
    "                                                   corpus=tfidf[bow_corpus_19], \n",
    "                                                   texts=small_tok.normalized_tokens,\n",
    "                                                   start=4, limit=11, step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ca2930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = models_07 + models_13 + models_19\n",
    "names = ['tm_{}_{}'.format(yr, tp) for yr in ['07', '13', '19'] for tp in ['04', '06', '08', '10']]\n",
    "\n",
    "for model, filename in zip(all_models, names):\n",
    "    topicsDict = {}\n",
    "    for topicNum in range(model.num_topics):\n",
    "        topicWords = [w for w, p in model.show_topic(topicNum)]\n",
    "        topicsDict['Topic_{}'.format(topicNum)] = topicWords\n",
    "\n",
    "    wordRanksDF = pd.DataFrame(topicsDict)\n",
    "    wordRanksDF.to_csv('../Data/Environmental Discourse/Single-Year-TMs/Top-Words/' + filename + '.pkl')\n",
    "    model.save('../Data/Environmental Discourse/Single-Year-TMs/Models/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8984f49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence = coherence_07 + coherence_13 + coherence_19\n",
    "coh = pd.DataFrame({'model': names, 'coherence': coherence})\n",
    "coh.to_pickle('../Data/Environmental Discourse/Single-Year-TMS/coherence_scores.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "40a3c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.save('../Data/Environmental Discourse/Single-Year-TMs/dictionary')\n",
    "gensim.corpora.MmCorpus.serialize('../Data/Environmental Discourse/Single-Year-TMs/bow_corpus.mm', bow_corpus)\n",
    "tfidf.save('../Data/Environmental Discourse/Single-Year-TMs/tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "15425cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = models_07 + models_13 + models_19\n",
    "names = ['tm_07_02.pkl',\n",
    "        'tm_07_04.pkl',\n",
    "        'tm_07_06.pkl',\n",
    "        'tm_07_08.pkl',\n",
    "        'tm_07_10.pkl',\n",
    "        'tm_13_02.pkl',\n",
    "        'tm_13_04.pkl',\n",
    "        'tm_13_06.pkl',\n",
    "        'tm_13_08.pkl',\n",
    "        'tm_13_10.pkl',\n",
    "        'tm_19_02.pkl',\n",
    "        'tm_19_04.pkl',\n",
    "        'tm_19_06.pkl',\n",
    "        'tm_19_08.pkl',\n",
    "        'tm_19_10.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560c5412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f58bef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.ldamodel.LdaModel(corpus=tfidf[bow_corpus_07],\n",
    "                                 id2word=dictionary,\n",
    "                                 num_topics=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "76ca4c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model = models.coherencemodel.CoherenceModel(model=model, \n",
    "                                                       corpus=tfidf[bow_corpus],\n",
    "                                                       dictionary=dictionary,\n",
    "                                                       coherence='u_mass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "65c56898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-11.51415597567715"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_model.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ac59e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
